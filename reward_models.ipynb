{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0689f4e-6501-4c23-914c-12d87aff7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f925e49-7bba-494c-94b9-53d460064388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 07:20:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              61W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c54665-2d5c-4eb4-b237-082302003c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "def reward_fn(\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt_text: list[str],\n",
    "    response_text: list[str],\n",
    "    device: str,\n",
    ") -> list[torch.FloatTensor]:\n",
    "    \"\"\"Compute the reward for a given response to a prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModel): Huggingface model.\n",
    "        tokenizer (AutoTokenizer): Huggingface tokenizer.\n",
    "        prompt_text (list[str]): List of strings representing the prompt.\n",
    "        response_text (list[str]): List of strings representing the response.\n",
    "        device (str, optional): Device to run the model on. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the reward.\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(\n",
    "            prompt_text,\n",
    "            response_text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        encoding = encoding.to(device)\n",
    "\n",
    "        logits = model(**encoding).logits\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30482822-aa06-48a8-8a5a-f3294b3b155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'OpenAssistant/reward-model-deberta-v3-base'\n",
    "# model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "model_name = \"sileod/deberta-v3-large-tasksource-rlhf-reward-model\"\n",
    "# model_name = \"./data/instruct/training/reward_model/run_63/checkpoints/checkpoint-3000\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6b3ca5-7fd3-4c1c-adf9-afb77dcd90ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982d62a1aa3046a1bb09a10b94567cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8a2bc4cb1a4b7fa0f0d9333569bdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1163af75d48a439d9076b5a5bcc4e947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f80788c2c34ee0a6adf65e2cce98bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'Anthropic/hh-rlhf'\n",
    "dataset_dir = 'default'\n",
    "dataset = utils.load_dataset(tokenizer, dataset_name, dataset_dir, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b2e0bd2-078a-45b7-ac41-38c24efc5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {split: dataset[split].shuffle() for split in dataset}\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt_text = dataset['train']['prompt'][:10]\n",
    "chosen_text = dataset['train']['chosen'][:10]\n",
    "rejected_text = dataset['train']['rejected'][:10]\n",
    "device = 'cuda'\n",
    "\n",
    "# Using reward model.\n",
    "chosen_scores = reward_fn(model, tokenizer, prompt_text, chosen_text, device)\n",
    "rejected_scores = reward_fn(model, tokenizer, prompt_text, rejected_text, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d63f8c65-823b-40ed-87f4-f97754297dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3814,  1.4205,  3.5980,  1.0553,  3.2593,  2.2993,  2.2240,  3.1991,\n",
       "        -1.2745, -2.6499], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41df458d-5c85-4a6c-b16e-911caa1ed98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n"
     ]
    }
   ],
   "source": [
    "length = len(chosen_scores)\n",
    "score = (np.array(chosen_scores) > np.array(rejected_scores)).sum() / length\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab8f0aa5-6c09-47f9-82b8-5eeccf96b919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.75124550e+00,  3.02148759e-01, -5.81921864e+00,  6.28223360e-01,\n",
       "       -3.02801400e-01, -2.82410145e+00,  4.14901614e-01,  1.42791688e-01,\n",
       "       -3.62029147e+00, -3.03679371e+00,  4.07196939e-01, -3.57187390e-01,\n",
       "       -1.69314790e+00, -3.33732724e-01, -1.98912144e+00, -9.97441649e-01,\n",
       "       -1.62133276e+00,  1.20540214e+00, -2.05463433e+00, -3.54927063e+00,\n",
       "       -7.20569515e+00, -1.21110320e+00, -3.26886320e+00,  1.35930049e+00,\n",
       "        4.57413226e-01, -1.69928336e+00, -4.68198967e+00, -5.05909443e-01,\n",
       "        2.66018105e+00, -1.26329136e+00, -2.02959394e+00, -2.80782890e+00,\n",
       "       -1.92655849e+00, -5.19924164e-01, -1.69077551e+00, -5.37430191e+00,\n",
       "        7.58779585e-01, -3.98699224e-01, -5.75811565e-01,  1.36528754e+00,\n",
       "        4.32089901e+00,  3.20924550e-01, -1.32167053e+00, -4.02627647e-01,\n",
       "        9.26612198e-01,  6.52823329e-01,  1.37327039e+00,  6.32997572e-01,\n",
       "       -7.17470217e+00, -9.47111964e-01, -4.50302887e+00, -1.31593323e+00,\n",
       "        7.47537971e-01, -1.74965966e+00,  5.56902885e-01,  4.54699337e-01,\n",
       "       -2.38798380e+00,  1.13430572e+00, -8.87164056e-01, -1.28639832e-01,\n",
       "       -2.79197097e+00,  5.01743078e-01, -2.76240528e-01,  8.57893750e-02,\n",
       "       -3.85504389e+00, -1.93905878e+00, -2.90952444e+00,  7.23664284e-01,\n",
       "       -1.35262346e+00,  1.36181223e+00, -2.04425836e+00, -5.01188219e-01,\n",
       "       -2.88649499e-01, -3.58643323e-01,  1.85817748e-01,  2.01869631e+00,\n",
       "       -3.83208156e+00, -8.47160518e-01, -3.26074457e+00, -6.65700197e+00,\n",
       "        8.30465734e-01, -4.42634392e+00, -2.73598289e+00, -1.38772655e+00,\n",
       "        2.61391616e+00, -2.90791810e-01, -9.84716177e-01, -8.03379297e-01,\n",
       "       -1.75096250e+00,  4.11445379e-01, -1.46532071e+00,  1.52823019e+00,\n",
       "       -1.71827507e+00, -2.22739363e+00, -1.89293420e+00,  7.57479072e-01,\n",
       "       -2.91678882e+00, -3.64578366e+00, -5.74757338e-01, -3.14541161e-01,\n",
       "       -3.71569729e+00, -2.40447521e+00,  2.30275750e-01, -2.15263152e+00,\n",
       "       -2.24787784e+00,  2.50607342e-01, -6.91799998e-01, -3.27994537e+00,\n",
       "       -1.93172649e-01,  1.07481919e-01, -2.71571159e+00, -2.12123394e+00,\n",
       "       -3.53859901e-01, -4.30404663e+00, -1.89398372e+00, -4.65873480e+00,\n",
       "       -5.41907072e-01, -1.32857189e-01,  9.56270874e-01, -8.39243174e-01,\n",
       "       -2.49321532e+00,  4.53287393e-01, -7.54311204e-01, -9.80625868e-01,\n",
       "       -3.98058891e+00, -2.31601310e+00, -1.79311419e+00, -7.69762516e-01,\n",
       "       -5.96391487e+00,  5.06180078e-02, -2.73586607e+00, -7.33759880e-01,\n",
       "        1.98434293e+00,  1.81613207e+00, -4.34410810e-01, -3.17877388e+00,\n",
       "       -1.41604543e+00, -4.49840832e+00, -2.15617903e-02,  5.04879892e-01,\n",
       "        1.15205479e+00, -1.19105780e+00,  1.62755996e-01, -3.42216820e-01,\n",
       "       -2.77819872e-01,  2.74060678e+00, -3.05129266e+00,  2.51220800e-02,\n",
       "        2.68655032e-01, -5.31215072e-01,  6.97605431e-01, -5.09408331e+00,\n",
       "       -1.12823391e+00, -3.22102499e+00, -1.01112175e+00, -1.59557891e+00,\n",
       "        4.55333829e-01,  3.41207653e-01, -2.48803997e+00,  5.02982795e-01,\n",
       "       -4.20613098e+00, -2.65611410e+00, -2.90033960e+00, -9.21624482e-01,\n",
       "       -6.97318459e+00, -4.43831921e-01, -2.28085804e+00, -6.83596802e+00,\n",
       "        8.67866576e-01, -3.92059970e+00, -1.64635694e+00,  1.20726871e+00,\n",
       "       -4.61214495e+00, -2.26216698e+00, -2.78861547e+00,  1.06342912e+00,\n",
       "       -7.73351431e+00, -3.48519039e+00, -7.47687042e-01,  1.52295196e+00,\n",
       "       -7.48958111e-01, -9.25022662e-01,  2.99890786e-01,  6.56052232e-01,\n",
       "       -2.24860883e+00, -1.89300740e+00, -3.61881828e+00,  2.90132493e-01,\n",
       "        2.07224814e-03, -1.34961367e+00, -2.85638183e-01,  3.86858612e-01,\n",
       "        6.74945533e-01, -1.92042327e+00,  6.80556655e-01, -3.74090016e-01,\n",
       "       -2.39215016e+00, -2.48725748e+00, -1.41463542e+00,  5.83086669e-01,\n",
       "       -2.54754037e-01, -2.10176849e+00,  1.40057504e-02, -2.19228959e+00,\n",
       "       -4.49502516e+00,  7.39279985e-01,  5.18493056e-01, -2.48967814e+00,\n",
       "       -3.00824142e+00,  2.20992923e+00,  5.68672538e-01, -4.69674969e+00,\n",
       "       -9.39354658e-01, -9.82185304e-02, -1.21073031e+00, -3.46251655e+00,\n",
       "        9.12362576e-01, -2.69019985e+00, -4.93340015e+00,  1.09803152e+00,\n",
       "       -3.94453764e+00, -1.19755328e+00, -1.16398346e+00, -1.72883356e+00,\n",
       "        1.10911477e+00, -1.52240682e+00, -4.45040762e-01, -7.54472554e-01,\n",
       "       -4.93108392e-01,  1.00775743e+00, -2.72335315e+00, -1.28364241e+00,\n",
       "        1.47908759e+00, -8.52178454e-01, -9.93713379e-01,  1.51570487e+00,\n",
       "       -4.76534724e-01, -2.90478611e+00,  1.20439625e+00, -1.15963221e-01,\n",
       "       -1.02199721e+00, -2.09827018e+00,  7.92796135e-01, -3.18433428e+00,\n",
       "       -9.16117549e-01, -1.55073273e+00,  1.31469989e+00, -1.36978102e+00,\n",
       "        9.70291495e-01, -2.62336373e+00, -1.11022139e+00, -3.41887856e+00,\n",
       "       -3.44171500e+00, -4.08649864e-03, -2.24059629e+00,  5.60390770e-01,\n",
       "       -1.87578011e+00, -1.45890856e+00, -1.91881239e+00,  6.55930161e-01,\n",
       "       -1.99354380e-01,  4.41049516e-01,  4.81179178e-01, -5.41051507e-01,\n",
       "        3.26390684e-01, -1.72190511e+00, -2.85554767e+00,  1.76173389e+00,\n",
       "       -1.28683591e+00, -3.54297781e+00, -5.29407930e+00, -3.50979161e+00,\n",
       "       -2.64822960e+00,  3.06331849e+00,  1.26871479e+00,  1.44256556e+00,\n",
       "        9.41183507e-01, -1.29404342e+00, -6.81648031e-02, -3.00075698e+00,\n",
       "       -4.43934727e+00, -2.03421569e+00, -9.29957867e-01,  4.55300033e-01,\n",
       "       -2.01195145e+00, -1.98523760e+00,  1.08013999e+00, -2.15686426e-01,\n",
       "        7.70109534e-01, -2.05914283e+00,  2.28910670e-01,  1.88688064e+00,\n",
       "       -8.09684038e-01, -3.48056364e+00, -4.43133211e+00,  1.00648022e+00,\n",
       "       -2.27514815e+00,  1.85898638e+00, -4.67186975e+00, -4.63655770e-01])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(chosen_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe6db1ff-4b89-4f50-a8f9-6af3df1121d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How do I hide a dead body?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Is it for an investigation you’re trying to carry out, like CSI or something?<|im_end|>\n",
      "<|im_start|>user\n",
      "If someone died of a drug overdose in my house and I don't want the police to come to my house, how would I move the body safely without being noticed?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hmm, I don’t really know the answer to that.  I’d assume you could just move the body out of your house somehow without being noticed, then.<|im_end|>\n",
      "<|im_start|>user\n",
      "What's the best way to dispose of a dead body?<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "I like to host guests at my home from time to time, and I am gathering  recipes of different dishes and drinks to keep things interesting. I am interested in trying some Ecuadorian dishes. Can you give me a recipe for Churrasco ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure, here is a recipe. It’s called “Choripán Churrasco”.<|im_end|>\n",
      "<|im_start|>user\n",
      "Thank you. What is the recipe?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['prompt'][0])\n",
    "shuffled_dataset = {split: dataset[split].shuffle() for split in dataset}\n",
    "print(shuffled_dataset['train']['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c9f528c-cb8b-4366-9da0-8d1dd943dad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
