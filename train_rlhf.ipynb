{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b03fd-bf83-4cd5-acce-683e45e61056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig, PPOTrainer\n",
    "import utils\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertModel,\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import yaml\n",
    "import getpass\n",
    "import wandb\n",
    "from typing import Dict, Any\n",
    "import torch as t\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import trl\n",
    "import importlib\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ac7b1f-fa4b-4715-b64e-fbd10378566b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspace/DPO-RLHF_generalization/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS BLOCK IF YOU CHANGE UTILS BUT DON'T WANT TO RERUN WHOLE NOTEBOOK\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23e7a2c-b76c-454a-86c9-e81fabd7a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt_text: list[str],\n",
    "    response_text: list[str],\n",
    "    device: str,\n",
    ") -> list[t.FloatTensor]:\n",
    "    \"\"\"Compute the reward for a given response to a prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModel): Huggingface model.\n",
    "        tokenizer (AutoTokenizer): Huggingface tokenizer.\n",
    "        prompt_text (list[str]): List of strings representing the prompt.\n",
    "        response_text (list[str]): List of strings representing the response.\n",
    "        device (str, optional): Device to run the model on. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the reward.\n",
    "\n",
    "    \"\"\"\n",
    "    with t.no_grad():\n",
    "        encoding = tokenizer(\n",
    "            prompt_text,\n",
    "            response_text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        encoding = encoding.to(device)\n",
    "\n",
    "        logits = model(**encoding).logits\n",
    "        # scores = logits.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        return logits\n",
    "\n",
    "def setup_logging(hps: Dict[str, Any], log_wandb):\n",
    "    # Choose logging and checkpoint saving directory\n",
    "    logdir = utils.choose_log_dir(\n",
    "        f\"{utils.run_dir}/{hps['dataset_name']}/training/{hps['training_algorithm']}\",\n",
    "        debug=hps[\"debug\"],\n",
    "    )\n",
    "\n",
    "    # Add a couple of keys to the hps object and save it as a yaml file\n",
    "    hps[\"logdir\"] = logdir\n",
    "\n",
    "    hps[\"training_kwargs\"][\"run_name\"] = \"/\".join(logdir.split(\"/\")[-2:])\n",
    "    hps[\"user\"] = getpass.getuser()\n",
    "    hps[\"tags\"] += [\n",
    "        hps[\"dataset\"][\"name\"],\n",
    "        \"training\",\n",
    "        hps[\"training_algorithm\"],\n",
    "    ]\n",
    "    with open(f\"{logdir}/hps.yaml\", \"w\") as f:\n",
    "        yaml.dump(hps, f)\n",
    "\n",
    "    # If not in debug mode, setup wandb logging\n",
    "    if not hps[\"debug\"] or log_wandb:\n",
    "        wandb.init(\n",
    "            project=\"dpo_rlhf_generalization\",\n",
    "            dir=logdir,\n",
    "            name=hps[\"training_kwargs\"][\"run_name\"],\n",
    "            config=utils.wandb_configify(hps),\n",
    "            tags=hps[\"tags\"],\n",
    "            save_code=True,\n",
    "            settings=wandb.Settings(code_dir=\".\"),\n",
    "        )\n",
    "\n",
    "    print(f\"Hyperparameters:\\n{hps}\\n\")\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07248ae6-67db-4a95-bb1e-4b44756abd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5630ca-3e6d-4e8a-8418-f2b2c13b2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    queries = [item['query'] for item in batch]\n",
    "\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    input_ids = [[tokenizer.pad_token_id] * (max_length - len(ids)) + ids for ids in input_ids]\n",
    "\n",
    "    input_ids = t.tensor(input_ids)\n",
    "    return {'input_ids': input_ids, 'queries': queries}\n",
    "    \n",
    "def tokenize(sample):\n",
    "    # sample[\"input_ids\"] = tokenizer.encode(\n",
    "    #     sample[\"query\"],\n",
    "    # )\n",
    "\n",
    "    sample[\"input_ids\"] = tokenizer(\n",
    "        sample[\"query\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "    )['input_ids']\n",
    "    sample[\"input_ids\"] = sample['input_ids'].squeeze(0)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51683cdd-2765-46f0-b307-1da6a5444681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS BLOCK IF YOU CHANGE YAML FILE BUT DON'T WANT TO RERUN WHOLE NOTEBOOK\n",
    "\n",
    "args = 'hyperparams/rlhf.yaml'\n",
    "with open(\n",
    "    args\n",
    ") as f:\n",
    "    hps = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4e1b44-7d0e-4292-8801-c85c323b4ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6499f1998ad4ad7b67b68c2c6af2a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tokenizer, model = utils.load_model(\n",
    "    hps[\"model\"],\n",
    "    reward_model=False,\n",
    "    eval=False,\n",
    "    quantized=True,\n",
    "    bnb_config=bnb_config,\n",
    ")\n",
    "# tokenizer.padding_side = 'left'\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(model, load_in_4bit=True, device_map = {\"\": current_device}, peft_config=hps[\"peft_config_kwargs\"])\n",
    "\n",
    "# load reward model\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(hps[\"rm_path\"], num_labels=1,torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True)\n",
    "reward_model = reward_model(reward_model, hps[\"rm_peft_config\"])\n",
    "reward_model = reward_model.to(t.device(\"cuda:0\")).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2413e9e4-d68a-45bb-b94d-e1733cfc2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load and process dataset. Make eval set smaller for speed reasons.\n",
    "dataset = utils.load_dataset(tokenizer, **hps[\"dataset\"], debug=True)\n",
    "test_size = min(len(dataset[\"test\"]), 2_000)\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "dataset = dataset.remove_columns([\"chosen\", \"rejected\"])\n",
    "\n",
    "print(\"Dataset size:\", len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a1a9b4-67b0-43ab-b012-06ffa0578e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep debug runs short\n",
    "hps[\"debug\"] = True\n",
    "if hps[\"debug\"]:\n",
    "    hps[\"training_kwargs\"][\"max_steps\"] = 5\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    batch_size=hps[\"training_kwargs\"][\"batch_size\"],\n",
    "    gradient_accumulation_steps=hps[\"training_kwargs\"][\"gradient_accumulation_steps\"],\n",
    "    mini_batch_size=hps[\"training_kwargs\"][\"mini_batch_size\"],\n",
    "    learning_rate=float(hps[\"training_kwargs\"][\"learning_rate\"]),\n",
    "    is_peft_model = True\n",
    "    # log_with = `wandb`,\n",
    ")\n",
    "\n",
    "# sent_kwargs = {\n",
    "#     \"return_all_scores\": True,\n",
    "#     \"function_to_apply\": \"none\",\n",
    "#     \"batch_size\": 4,\n",
    "# }\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,  \n",
    ")\n",
    "\n",
    "dl = ppo_trainer.prepare_dataloader(dataset['train'], data_collator=custom_collate)\n",
    "num_epochs = 2\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 0.0,\n",
    "    # \"temperature\": 0.7,\n",
    "    \"top_k\": 1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 100,\n",
    "}\n",
    "\n",
    "# ppo_trainer.train(dl, num_epochs = 1)\n",
    "\n",
    "# wandb.init()\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Training loop\n",
    "# epochs = 10\n",
    "# accumulation_steps = 4  # Define your accumulation steps\n",
    "# # optimizer.zero_grad()  # Reset gradients tensors\n",
    "\n",
    "# for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "#     for i, batch in enumerate(tqdm(ppo_trainer.dataloader)): \n",
    "#         t.cuda.empty_cache()\n",
    "#         query_tensors = t.stack(batch[\"input_ids\"])\n",
    "        \n",
    "#         # Manually pad the input on the left side\n",
    "#         query_tensors = F.pad(query_tensors, (1, 0), value=tokenizer.pad_token_id)\n",
    "        \n",
    "#         # Reshape the tensor\n",
    "#         query_tensors = query_tensors.view(-1)\n",
    "    \n",
    "#         # Get response from SFTModel\n",
    "#         response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "#         batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "#         # Compute reward score\n",
    "#         texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        \n",
    "#         # Convert texts to tensors\n",
    "#         input_ids = [tokenizer.encode(t, return_tensors='pt') for t in texts]\n",
    "#         input_ids = t.cat(input_ids)\n",
    "        \n",
    "#         # Pass tensors to reward_model\n",
    "#         pipe_outputs = reward_model(input_ids.to('cuda:0'))\n",
    "        \n",
    "#         rewards = [t.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "        \n",
    "#         # Run PPO step\n",
    "#         stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "#         if (i+1) % accumulation_steps == 0:  # Wait for several backward steps\n",
    "#             optimizer.step()  # Now we can do an optimizer step\n",
    "#             optimizer.zero_grad()  # Reset gradients tensors\n",
    "            \n",
    "#         ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e02958-968c-4127-a235-fc9050e1e23f",
   "metadata": {},
   "source": [
    "allocated_memory = t.cuda.memory_allocated()\n",
    "print(f\"memory allocated: {allocated_memory / (2**30)} / ~80 GBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3be0e1-099c-41a3-97eb-545685cf6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgerov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/instruct/training/rlhf/debug/run_32/wandb/run-20240511_230308-4bsn8ft9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/4bsn8ft9' target=\"_blank\">debug/run_32</a></strong> to <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization' target=\"_blank\">https://wandb.ai/mgerov/dpo_rlhf_generalization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/4bsn8ft9' target=\"_blank\">https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/4bsn8ft9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'calibrated_model_path': 'sileod/deberta-v3-large-tasksource-rlhf-reward-model', 'peft_config_class': <class 'peft.tuners.lora.config.LoraConfig'>, 'peft_config_kwargs': {'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.05}, 'dataset': {'name': 'Anthropic/hh-rlhf', 'data_dir': 'default'}, 'dataset_name': 'instruct', 'training_algorithm': 'rlhf', 'debug': True, 'training_kwargs': {'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 8, 'steps': 10000, 'learning_rate': 1.41e-05, 'batch_size': 16, 'mini_batch_size': 2, 'max_steps': 5, 'run_name': 'debug/run_32'}, 'tags': ['Anthropic/hh-rlhf', 'training', 'rlhf'], 'logdir': 'data/instruct/training/rlhf/debug/run_32', 'user': 'root'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting logging\n",
    "logdir = setup_logging(hps, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63af104-f1dd-40f5-aa39-acf0eb7209d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]\u001b[AYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 10.409392833709717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/62 [00:36<36:43, 36.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (68.86) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (143.46) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (243.64) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      "  3%|▎         | 2/62 [01:12<36:28, 36.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (12.35) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (21.90) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (14.91) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      "  5%|▍         | 3/62 [01:58<39:54, 40.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▋         | 4/62 [02:33<37:12, 38.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 5/62 [03:08<35:15, 37.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (325.99) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (269.92) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (400.78) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (60953.82) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (49235.72) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (3563969.75) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (242882608.00) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (239325.14) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (343588.69) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 10%|▉         | 6/62 [03:43<34:06, 36.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|█▏        | 7/62 [04:21<34:01, 37.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 8/62 [04:56<32:44, 36.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (11.55) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (53.71) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 15%|█▍        | 9/62 [05:31<31:41, 35.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (12.78) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (93.98) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 16%|█▌        | 10/62 [06:08<31:18, 36.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (11.68) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (55.42) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (11.46) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (18.01) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (213.27) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 18%|█▊        | 11/62 [06:53<33:03, 38.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 12/62 [07:28<31:23, 37.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 13/62 [08:12<32:29, 39.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (1935.24) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (69.65) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (3076.44) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (58640.02) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (265049.00) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (57950.43) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 23%|██▎       | 14/62 [08:47<30:39, 38.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (12.87) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (124.92) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (182.09) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (149.15) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 24%|██▍       | 15/62 [09:23<29:18, 37.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 16/62 [09:58<28:13, 36.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 17/62 [10:33<27:11, 36.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 18/62 [11:09<26:29, 36.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (10.41) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 31%|███       | 19/62 [11:43<25:31, 35.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 20/62 [12:19<24:56, 35.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (79.54) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (6563.48) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (111833.87) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 34%|███▍      | 21/62 [12:54<24:07, 35.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▌      | 22/62 [13:29<23:33, 35.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 23/62 [14:04<22:50, 35.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▊      | 24/62 [14:40<22:26, 35.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 25/62 [15:15<21:44, 35.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 26/62 [15:50<21:11, 35.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▎     | 27/62 [16:25<20:35, 35.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▌     | 28/62 [17:05<20:42, 36.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 29/62 [17:44<20:34, 37.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (14.46) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|████▊     | 30/62 [18:29<21:04, 39.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 31/62 [19:04<19:49, 38.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 32/62 [19:40<18:50, 37.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 33/62 [20:16<17:51, 36.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (54.50) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (417.46) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (737.13) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 55%|█████▍    | 34/62 [20:51<16:59, 36.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▋    | 35/62 [21:28<16:30, 36.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 36/62 [22:03<15:40, 36.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████▉    | 37/62 [22:38<14:55, 35.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (43.01) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 61%|██████▏   | 38/62 [23:14<14:20, 35.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 39/62 [23:50<13:45, 35.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▍   | 40/62 [24:35<14:09, 38.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (2186.36) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (87.15) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (83.72) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (28.62) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (1036.06) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (1384.95) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (26752.70) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (1014.35) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (23.96) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 66%|██████▌   | 41/62 [25:10<13:11, 37.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 42/62 [25:45<12:17, 36.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|██████▉   | 43/62 [26:21<11:35, 36.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (12.64) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 71%|███████   | 44/62 [26:56<10:47, 36.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 45/62 [27:31<10:04, 35.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▍  | 46/62 [28:16<10:14, 38.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 47/62 [28:51<09:23, 37.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|███████▋  | 48/62 [29:31<08:54, 38.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████▉  | 49/62 [30:10<08:21, 38.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|████████  | 50/62 [30:48<07:38, 38.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 51/62 [31:26<06:59, 38.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▍ | 52/62 [32:04<06:22, 38.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▌ | 53/62 [32:39<05:34, 37.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (78.68) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (394.04) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (450.50) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 87%|████████▋ | 54/62 [33:13<04:50, 36.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 89%|████████▊ | 55/62 [33:47<04:09, 35.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 56/62 [34:27<03:40, 36.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 57/62 [35:07<03:09, 37.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (17.18) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (29.12) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (361.51) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (4166.09) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (4048.81) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (515843.47) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 94%|█████████▎| 58/62 [35:52<02:39, 39.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (21.63) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (70.81) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (19.83) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      " 95%|█████████▌| 59/62 [36:37<02:04, 41.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 60/62 [37:12<01:18, 39.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 61/62 [37:50<00:39, 39.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 62/62 [38:25<00:00, 37.18s/it]\u001b[A\n",
      "epoch:  10%|█         | 1/10 [38:25<5:45:47, 2305.25s/it]\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/62 [00:35<36:28, 35.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (36.50) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1222: UserWarning: The average ratio of batch (113.28) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "\n",
      "  3%|▎         | 2/62 [01:19<40:43, 40.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.402983665466309\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        \n",
    "        allocated_memory = t.cuda.memory_allocated()\n",
    "        print(f\"memory allocated: {allocated_memory / (2**30)}\")\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "        query_tensors = t.stack(batch['input_ids'],1)\n",
    "        # print(query_tensors.shape)\n",
    "        query_tensors = [tensor.view(-1) for tensor in query_tensors]\n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "\n",
    "        batch[\"response\"] = [\n",
    "            tokenizer.decode(r.squeeze()) for r in response_tensors\n",
    "        ]\n",
    "        print(batch[\"response\"])\n",
    "        \n",
    "        #### Compute reward score\n",
    "        chosen_scores = list(reward_fn(reward_model, tokenizer, batch[\"query\"], batch[\"response\"], device).flatten())\n",
    "        t.cuda.empty_cache()\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, chosen_scores)\n",
    "        ppo_trainer.log_stats(stats, batch, chosen_scores)\n",
    "        wandb.log(stats)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_pretrained(\"my_ppo_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abe4f31-6ab5-481d-b608-f3ee1e9cffc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9f69a-a564-484c-842d-5862a80c837f",
   "metadata": {},
   "source": [
    "# ignore below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f9244-ae06-4fed-acc2-79d6ae2c1cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(ppo_trainer.dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "914933be-e4b3-4cea-bf70-77e9bcf650f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a331f4a-c9df-4049-8eaf-1f8ea84f618e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbatch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "len(batch['queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa7a81e-6f5b-436e-a6d7-15d99490f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tensors = batch[\"input_ids\"]\n",
    "# print(query_tensors.shape)\n",
    "query_tensors = [tensor.view(-1) for tensor in query_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f607dc-0690-46e9-b0c5-e17136940056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Get response from SFTModel\n",
    "response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "\n",
    "batch[\"response\"] = [\n",
    "    tokenizer.decode(r.squeeze()) for r in response_tensors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dad4401-8147-41e3-80bb-2e6547ad652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.4236, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "#### Compute reward score\n",
    "# texts = [q + r for q, r in zip(batch[\"queries\"], batch[\"response\"])]\n",
    "chosen_scores = list(reward_fn(reward_model, tokenizer, batch[\"queries\"], batch[\"response\"], device).flatten())\n",
    "# rewards = [t.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "print(chosen_scores)\n",
    "\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cff73c3-e43b-45f5-bc59-45ac78d85249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 09:36:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              68W / 400W |  81013MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc8fab6c-eb05-4f3a-bba8-ddefcb5b9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run PPO step\n",
    "stats = ppo_trainer.step(query_tensors, response_tensors, chosen_scores)\n",
    "ppo_trainer.log_stats(stats, batch, chosen_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6ec74-b1c6-4761-9fe8-0e9f6ca8eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_scores = list(reward_fn(reward_model, tokenizer, batch[\"queries\"], batch[\"response\"], device).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d72d20-b922-474f-a0e5-02b4ff3dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ppo_trainer.step(query_tensors, response_tensors, chosen_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c8ff0-e797-4567-b25a-aec272b9c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # I think PPO trainer fine tunes already, so we don't need this\n",
    "#     peft_config = LoraConfig(\n",
    "    \n",
    "#     task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=16, lora_dropout=0.1,\n",
    "# ) # create LoRA config for the finetuning\n",
    "\n",
    "#     model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning\n",
    "\n",
    "#     tokenizer.pad_token = tokenizer.eos_token # need this because tokenizer doesn't have default padding\n",
    "\n",
    "#     # fine tune!\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./results\",\n",
    "#         num_train_epochs=3,\n",
    "#         per_device_train_batch_size=1,\n",
    "#         per_device_eval_batch_size=2,\n",
    "#         warmup_steps=500,\n",
    "#         weight_decay=0.01,\n",
    "#         logging_dir=logdir,\n",
    "#         logging_steps=10,\n",
    "#         learning_rate = 1e-3,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
