{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5459ac0e-661f-4b5a-9ec5-7c0adcf68df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Type, Union, Tuple, Literal, List\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "import getpass\n",
    "import datasets\n",
    "import peft\n",
    "import transformers\n",
    "import gc\n",
    "import trl\n",
    "import wandb\n",
    "import utils\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d73b12f-9043-4cad-bb43-28c8b28c847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 03:34:20 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:2D:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              41W / 300W |     18MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d198ea0d-564c-496f-8f82-b06c207ba5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THIS FILE TRAINS EITHER DPO OR A REWARD MODEL GIVEN A DATASET -- Depending \n",
    " on which hyperparams you pass it. For instance to train DPO, run python train.py hyperparams/dpo.yaml\"\"\"\n",
    "\n",
    "def check_cuda_gpu_availability():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.get_device_name(0)\n",
    "        print(f\"Using CUDA GPU: {device}\")\n",
    "    else:\n",
    "        print(\"CUDA GPU is not available.\")\n",
    "\n",
    "\n",
    "class DPOTrainer(trl.DPOTrainer):\n",
    "    _tag_names = [\"trl\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[PreTrainedModel, nn.Module, str] = None,\n",
    "        ref_model: Optional[Union[PreTrainedModel, nn.Module, str]] = None,\n",
    "        alpha: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            ref_model=ref_model,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def dpo_loss(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.FloatTensor,\n",
    "        policy_rejected_logps: torch.FloatTensor,\n",
    "        reference_chosen_logps: torch.FloatTensor,\n",
    "        reference_rejected_logps: torch.FloatTensor,\n",
    "        reference_free: bool = False,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "        Args:\n",
    "            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "            reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n",
    "            The losses tensor contains the DPO loss for each example in the batch.\n",
    "            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n",
    "        \"\"\"\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "        pi_logratios = pi_logratios.to(self.accelerator.device)\n",
    "        ref_logratios = ref_logratios.to(self.accelerator.device)\n",
    "\n",
    "        logits = self.beta * pi_logratios - self.beta * ref_logratios\n",
    "\n",
    "        # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n",
    "        # We ignore the reference model as beta -> 0. The label_smoothing parameter encodes our uncertainty about the labels and\n",
    "        # calculates a conservative DPO loss.\n",
    "        if self.loss_type == \"sigmoid\":\n",
    "            losses = (\n",
    "                -F.logsigmoid(logits) * (1 - self.label_smoothing)\n",
    "                - F.logsigmoid(-logits) * self.label_smoothing\n",
    "            )\n",
    "\n",
    "        chosen_rewards = (\n",
    "            self.beta * policy_chosen_logps.to(self.accelerator.device)\n",
    "            - self.beta * reference_chosen_logps.to(self.accelerator.device)\n",
    "        ).detach()\n",
    "        rejected_rewards = (\n",
    "            self.beta * policy_rejected_logps.to(self.accelerator.device)\n",
    "            - self.beta * reference_rejected_logps.to(self.accelerator.device)\n",
    "        ).detach()\n",
    "\n",
    "        # Here we renormalize logps such that p(chosen) + p(rejected) = 1\n",
    "        # We then use this renormalized distribution to compute pairwise ce, entropy, and kl.\n",
    "        policy_logps = torch.stack([policy_chosen_logps, policy_rejected_logps], dim=0)\n",
    "        log_c_policy = -torch.logsumexp(policy_logps, dim=0)\n",
    "        renormed_policy_logps = policy_logps - log_c_policy\n",
    "\n",
    "        reference_logps = torch.stack(\n",
    "            [reference_chosen_logps, reference_rejected_logps], dim=0\n",
    "        )\n",
    "        log_c_reference = -torch.logsumexp(reference_logps, dim=0)\n",
    "        renormed_reference_logps = reference_logps - log_c_reference\n",
    "\n",
    "        ce = -(renormed_policy_logps.exp() * renormed_reference_logps).sum(dim=0)\n",
    "        entropy = -(renormed_policy_logps.exp() * renormed_policy_logps).sum(dim=0)\n",
    "        kl = ce - entropy\n",
    "\n",
    "        return losses, {\n",
    "            \"chosen_rewards\": chosen_rewards,\n",
    "            \"rejected_rewards\": rejected_rewards,\n",
    "            \"pairwise_cross_entropy\": ce,\n",
    "            \"pairwise_entropy\": entropy,\n",
    "            \"pairwise_kl\": kl,\n",
    "        }\n",
    "\n",
    "    def get_batch_loss_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        (\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            policy_chosen_logits,\n",
    "            policy_rejected_logits,\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "\n",
    "        # if reference_chosen_logps and reference_rejected_logps in batch use them, otherwise use the reference model\n",
    "        if \"reference_chosen_logps\" in batch and \"reference_rejected_logps\" in batch:\n",
    "            reference_chosen_logps = batch[\"reference_chosen_logps\"]\n",
    "            reference_rejected_logps = batch[\"reference_rejected_logps\"]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.ref_model is None:\n",
    "                    with self.null_ref_context():\n",
    "                        (\n",
    "                            reference_chosen_logps,\n",
    "                            reference_rejected_logps,\n",
    "                            _,\n",
    "                            _,\n",
    "                        ) = self.concatenated_forward(self.model, batch)\n",
    "                else:\n",
    "                    (\n",
    "                        reference_chosen_logps,\n",
    "                        reference_rejected_logps,\n",
    "                        _,\n",
    "                        _,\n",
    "                    ) = self.concatenated_forward(self.ref_model, batch)\n",
    "\n",
    "        losses, metrics_dict = self.dpo_loss(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps,\n",
    "        )\n",
    "        reward_accuracies = (\n",
    "            metrics_dict[\"chosen_rewards\"] > metrics_dict[\"rejected_rewards\"]\n",
    "        ).float()\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        metrics[f\"{prefix}rewards/chosen\"] = metrics_dict[\"chosen_rewards\"].mean().cpu()\n",
    "        metrics[f\"{prefix}rewards/rejected\"] = (\n",
    "            metrics_dict[\"rejected_rewards\"].mean().cpu()\n",
    "        )\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (\n",
    "            (metrics_dict[\"chosen_rewards\"] - metrics_dict[\"rejected_rewards\"])\n",
    "            .mean()\n",
    "            .cpu()\n",
    "        )\n",
    "        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logits/rejected\"] = (\n",
    "            policy_rejected_logits.detach().mean().cpu()\n",
    "        )\n",
    "        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}pairwise_cross_entropy\"] = (\n",
    "            metrics_dict[\"pairwise_cross_entropy\"].mean().cpu()\n",
    "        )\n",
    "        metrics[f\"{prefix}pairwise_entropy\"] = (\n",
    "            metrics_dict[\"pairwise_entropy\"].mean().cpu()\n",
    "        )\n",
    "        metrics[f\"{prefix}pairwise_kl\"] = metrics_dict[\"pairwise_kl\"].mean().cpu()\n",
    "\n",
    "        return losses.mean(), metrics\n",
    "\n",
    "\n",
    "def setup_logging(hps: Dict[str, Any]):\n",
    "    # Choose logging and checkpoint saving directory\n",
    "    if hps[\"dataset\"][\"name\"] == \"Unified-Language-Model-Alignment/Anthropic_HH_Golden\":\n",
    "    # \"Dahoas/synthetic-instruct-gptj-pairwise\":\n",
    "        hps[\"dataset_name\"] = \"instruct\"\n",
    "    logdir = utils.choose_log_dir(\n",
    "        f\"{utils.run_dir}/{hps['dataset_name']}/training/{hps['training_algorithm']}\",\n",
    "        debug=hps[\"debug\"],\n",
    "    )\n",
    "\n",
    "    # Add a couple of keys to the hps object and save it as a yaml file\n",
    "    hps[\"logdir\"] = logdir\n",
    "    hps[\"training_kwargs\"][\"run_name\"] = \"/\".join(logdir.split(\"/\")[-2:])\n",
    "    hps[\"user\"] = getpass.getuser()\n",
    "    hps[\"tags\"] += [\n",
    "        hps[\"dataset\"][\"name\"],\n",
    "        \"training\",\n",
    "        hps[\"training_algorithm\"],\n",
    "    ]\n",
    "    with open(f\"{logdir}/hps.yaml\", \"w\") as f:\n",
    "        yaml.dump(hps, f)\n",
    "\n",
    "    # If not in debug mode, setup wandb logging\n",
    "    if not hps[\"debug\"]:\n",
    "        wandb.init(\n",
    "            project=\"dpo_rlhf_generalization\",\n",
    "            dir=logdir,\n",
    "            name=hps[\"training_kwargs\"][\"run_name\"],\n",
    "            config=utils.wandb_configify(hps),\n",
    "            tags=hps[\"tags\"],\n",
    "            save_code=True,\n",
    "            settings=wandb.Settings(code_dir=\".\"),\n",
    "        )\n",
    "\n",
    "    print(f\"Hyperparameters:\\n{hps}\\n\")\n",
    "    return logdir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5ce06f-ebfe-42bb-b4e8-ad6315592e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "    dataset: datasets.DatasetDict,\n",
    "    logdir: str,\n",
    "    training_kwargs: Dict[str, Any],\n",
    "    training_algorithm: str,\n",
    "    training_algorithm_kwargs: Dict[str, Any],\n",
    "    debug: bool,\n",
    "    peft_config_class: Optional[Type[peft.PeftConfig]] = None,\n",
    "    peft_config_kwargs: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    # Build training args, which are independent of the specific training algorithm\n",
    "    args_class = (\n",
    "        trl.RewardConfig\n",
    "        if training_algorithm == \"reward_model\"\n",
    "        else transformers.TrainingArguments\n",
    "    )\n",
    "    train_args = args_class(\n",
    "        output_dir=f\"{logdir}/checkpoints\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        report_to=\"none\" if debug else \"wandb\",\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=10,\n",
    "        eval_steps=200,\n",
    "        save_steps=0.2,\n",
    "        **training_kwargs,\n",
    "    )\n",
    "\n",
    "    # Construct trainer\n",
    "    if training_algorithm in [\"dpo\"]:\n",
    "        trainer = DPOTrainer(\n",
    "            model,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            args=train_args,\n",
    "            max_prompt_length=1024,\n",
    "            max_length=1536,\n",
    "            peft_config=peft_config_class(**peft_config_kwargs),\n",
    "            **training_algorithm_kwargs,\n",
    "        )\n",
    "    elif training_algorithm == \"reward_model\":\n",
    "\n",
    "        def prep_for_reward_trainer(sample):\n",
    "            chosen = [p + c for p, c in zip(sample[\"prompt\"], sample[\"chosen\"])]\n",
    "            chosen_inputs = tokenizer(\n",
    "                chosen,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=1536,\n",
    "            )\n",
    "            rejected = [p + r for p, r in zip(sample[\"prompt\"], sample[\"rejected\"])]\n",
    "            rejected_inputs = tokenizer(\n",
    "                rejected,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=1536,\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids_chosen\": chosen_inputs[\"input_ids\"],\n",
    "                \"attention_mask_chosen\": chosen_inputs[\"attention_mask\"],\n",
    "                \"input_ids_rejected\": rejected_inputs[\"input_ids\"],\n",
    "                \"attention_mask_rejected\": rejected_inputs[\"attention_mask\"],\n",
    "            }\n",
    "\n",
    "        train_args.max_length = 1536\n",
    "        trainer = trl.RewardTrainer(\n",
    "            model,\n",
    "            train_dataset=dataset[\"train\"].map(prep_for_reward_trainer, batched=True),\n",
    "            eval_dataset=dataset[\"test\"].map(prep_for_reward_trainer, batched=True),\n",
    "            tokenizer=tokenizer,\n",
    "            args=train_args,\n",
    "            peft_config=peft_config_class(**peft_config_kwargs),\n",
    "            **training_algorithm_kwargs,\n",
    "        )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e7d63c-3cc7-46dc-8868-be4f7a14d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load hyperparameters\n",
    "    args = \"hyperparams/reward_model.yaml\"\n",
    "    with open(\n",
    "        args.hyperparam_file,\n",
    "    ) as f:\n",
    "        hps = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    # To keep debug runs short\n",
    "    hps[\"debug\"] = args.debug\n",
    "    if hps[\"debug\"]:\n",
    "        hps[\"training_kwargs\"][\"max_steps\"] = 5\n",
    "\n",
    "    tokenizer, model = utils.load_model(\n",
    "        hps[\"model\"],\n",
    "        reward_model=hps[\"training_algorithm\"] == \"reward_model\",\n",
    "        eval=False,\n",
    "    )\n",
    "\n",
    "    # Load and process dataset. Make eval set smaller for speed reasons.\n",
    "    dataset = utils.load_dataset(tokenizer, **hps[\"dataset\"], debug=args.debug)\n",
    "\n",
    "    # Setting logging\n",
    "    logdir = setup_logging(hps)\n",
    "\n",
    "    # Train\n",
    "    if hps[\"training_algorithm\"] == \"dpo\":\n",
    "        hps[\"training_algorithm_kwargs\"][\"alpha\"] = hps[\"training_algorithm_kwargs\"][\n",
    "            \"beta\"\n",
    "        ]\n",
    "    trainer = build_trainer(\n",
    "        tokenizer,\n",
    "        model,\n",
    "        dataset,\n",
    "        logdir,\n",
    "        hps[\"training_kwargs\"],\n",
    "        hps[\"training_algorithm\"],\n",
    "        hps[\"training_algorithm_kwargs\"],\n",
    "        peft_config_class=hps.get(\"peft_config_class\"),\n",
    "        peft_config_kwargs=hps.get(\"peft_config_kwargs\"),\n",
    "        debug=args.debug,\n",
    "    )\n",
    "\n",
    "    \n",
    "    trainer.train()\n",
    "    if not hps[\"debug\"]:\n",
    "        model.save_pretrained(f\"{logdir}/checkpoints/final\")\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2240512-5425-4aec-bed4-749d3c5e908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0156d02a-5e24-4eaf-bde3-6d1b4196d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e2da2863f64a4a850bb046cab09eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252c6a138fdb41c09107315357c41956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgerov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/instruct/training/reward_model/run_71/wandb/run-20240511_033445-2pq2eu0d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/2pq2eu0d' target=\"_blank\">reward_model/run_71</a></strong> to <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization' target=\"_blank\">https://wandb.ai/mgerov/dpo_rlhf_generalization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/2pq2eu0d' target=\"_blank\">https://wandb.ai/mgerov/dpo_rlhf_generalization/runs/2pq2eu0d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "{'model': 'mistralai/Mistral-7B-Instruct-v0.2', 'peft_config_class': <class 'peft.tuners.lora.config.LoraConfig'>, 'peft_config_kwargs': {'r': 16, 'lora_alpha': 16, 'lora_dropout': 0.05}, 'dataset': {'name': 'Anthropic/hh-rlhf', 'data_dir': 'default'}, 'dataset_name': 'instruct', 'training_algorithm': 'reward_model', 'training_algorithm_kwargs': {}, 'training_kwargs': {'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 8, 'gradient_accumulation_steps': 8, 'max_steps': 5000, 'learning_rate': 0.0001, 'run_name': 'reward_model/run_71'}, 'tags': ['Anthropic/hh-rlhf', 'training', 'reward_model'], 'debug': False, 'logdir': 'data/instruct/training/reward_model/run_71', 'user': 'root'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reward_model_path = \"./drive/root/project_data/calibrated_alignment/runs/instruct/training/reward_model/run_3/checkpoints/checkpoint-4000\"\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path, torch_dtype=t.bfloat16).eval()\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "args = \"hyperparams/reward_model.yaml\"\n",
    "with open(\n",
    "    args,\n",
    ") as f:\n",
    "    hps = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# To keep debug runs short\n",
    "hps[\"debug\"] = False\n",
    "\n",
    "if hps[\"debug\"]:\n",
    "    hps[\"training_kwargs\"][\"max_steps\"] = 5\n",
    "\n",
    "tokenizer, _ = utils.load_model(\n",
    "    hps[\"model\"],\n",
    "    reward_model=hps[\"training_algorithm\"] == \"reward_model\",\n",
    "    eval=hps[\"debug\"],\n",
    ")\n",
    "reward_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "# Load and process dataset. Make eval set smaller for speed reasons.\n",
    "dataset = utils.load_dataset(tokenizer, **hps[\"dataset\"], debug=hps[\"debug\"])\n",
    "\n",
    "# Setting logging\n",
    "logdir = setup_logging(hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8f4dfc-0931-4a99-9bda-b96031f7d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'] = dataset['test'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a712c506-eddd-43a6-a430-5796ac8c9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3d53ca1ad2465188b6ce87f475f7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(\n",
    "        tokenizer,\n",
    "        reward_model,\n",
    "        dataset,\n",
    "        logdir,\n",
    "        hps[\"training_kwargs\"],\n",
    "        hps[\"training_algorithm\"],\n",
    "        hps[\"training_algorithm_kwargs\"],\n",
    "        peft_config_class=hps.get(\"peft_config_class\"),\n",
    "        peft_config_kwargs=hps.get(\"peft_config_kwargs\"),\n",
    "        debug=hps[\"debug\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0adaf8c0-034c-46fb-9f16-7cd61ba44c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2692: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 04:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/utils.py:553: UserWarning: There are 13 out of 1000 instances where the predictions for both options are equal. As a consequence the accuracy can be misleading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.112468719482422,\n",
       " 'eval_accuracy': 0.522,\n",
       " 'eval_runtime': 272.5742,\n",
       " 'eval_samples_per_second': 3.669,\n",
       " 'eval_steps_per_second': 0.459}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07595340-386d-443e-82bd-791c4382c636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
