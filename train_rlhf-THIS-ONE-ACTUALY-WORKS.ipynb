{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b03fd-bf83-4cd5-acce-683e45e61056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOConfig, PPOTrainer\n",
    "import utils\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertModel,\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import yaml\n",
    "import getpass\n",
    "import wandb\n",
    "from typing import Dict, Any\n",
    "import torch as t\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "import trl\n",
    "import importlib\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ac7b1f-fa4b-4715-b64e-fbd10378566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 12 16:00:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100X                   On  | 00000000:C6:00.0 Off |                    0 |\n",
      "| N/A   56C    P0              70W / 300W |     21MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS BLOCK IF YOU CHANGE UTILS BUT DON'T WANT TO RERUN WHOLE NOTEBOOK\n",
    "!nvidia-smi\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23e7a2c-b76c-454a-86c9-e81fabd7a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(\n",
    "    model: AutoModel,\n",
    "    reward_tokenizer: AutoTokenizer,\n",
    "    prompt_text: list[str],\n",
    "    response_text: list[str],\n",
    "    device: str,\n",
    ") -> list[t.FloatTensor]:\n",
    "    \"\"\"Compute the reward for a given response to a prompt.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModel): Huggingface model.\n",
    "        tokenizer (AutoTokenizer): Huggingface tokenizer.\n",
    "        prompt_text (list[str]): List of strings representing the prompt.\n",
    "        response_text (list[str]): List of strings representing the response.\n",
    "        device (str, optional): Device to run the model on. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the reward.\n",
    "\n",
    "    \"\"\"\n",
    "    with t.no_grad():\n",
    "        encoding = reward_tokenizer(\n",
    "            prompt_text,\n",
    "            response_text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        encoding = encoding.to(device)\n",
    "\n",
    "        logits = model(**encoding).logits\n",
    "        # scores = logits.cpu().numpy().flatten().tolist()\n",
    "\n",
    "        return logits\n",
    "\n",
    "def setup_logging(hps: Dict[str, Any], log_wandb):\n",
    "    # Choose logging and checkpoint saving directory\n",
    "    logdir = utils.choose_log_dir(\n",
    "        f\"{utils.run_dir}/{hps['dataset_name']}/training/{hps['training_algorithm']}\",\n",
    "        debug=hps[\"debug\"],\n",
    "    )\n",
    "\n",
    "    # Add a couple of keys to the hps object and save it as a yaml file\n",
    "    hps[\"logdir\"] = logdir\n",
    "\n",
    "    hps[\"training_kwargs\"][\"run_name\"] = \"/\".join(logdir.split(\"/\")[-2:])\n",
    "    hps[\"user\"] = getpass.getuser()\n",
    "    hps[\"tags\"] += [\n",
    "        hps[\"dataset\"][\"name\"],\n",
    "        \"training\",\n",
    "        hps[\"training_algorithm\"],\n",
    "    ]\n",
    "    with open(f\"{logdir}/hps.yaml\", \"w\") as f:\n",
    "        yaml.dump(hps, f)\n",
    "\n",
    "    # If not in debug mode, setup wandb logging\n",
    "    if not hps[\"debug\"] or log_wandb:\n",
    "        wandb.init(\n",
    "            project=\"dpo_rlhf_generalization\",\n",
    "            dir=logdir,\n",
    "            name=hps[\"training_kwargs\"][\"run_name\"],\n",
    "            config=utils.wandb_configify(hps),\n",
    "            tags=hps[\"tags\"],\n",
    "            save_code=True,\n",
    "            settings=wandb.Settings(code_dir=\".\"),\n",
    "        )\n",
    "\n",
    "    print(f\"Hyperparameters:\\n{hps}\\n\")\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07248ae6-67db-4a95-bb1e-4b44756abd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5630ca-3e6d-4e8a-8418-f2b2c13b2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    queries = [item['query'] for item in batch]\n",
    "\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    input_ids = [[tokenizer.pad_token_id] * (max_length - len(ids)) + ids for ids in input_ids]\n",
    "\n",
    "    input_ids = t.tensor(input_ids)\n",
    "    return {'input_ids': input_ids, 'queries': queries}\n",
    "    \n",
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(\n",
    "        sample[\"query\"].replace(\"</s>\",\"\"),\n",
    "    )\n",
    "\n",
    "    # sample[\"input_ids\"] = tokenizer(\n",
    "    #     sample[\"query\"],\n",
    "    #     truncation=True,\n",
    "    #     max_length=512,\n",
    "    #     padding='max_length',\n",
    "    #     return_tensors='pt',\n",
    "    # )['input_ids']\n",
    "    # sample[\"input_ids\"] = sample['input_ids'].squeeze(0)\n",
    "    return sample\n",
    "\n",
    "def collator(data):\n",
    "    return {key: [d[key] for d in data] for key in data[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51683cdd-2765-46f0-b307-1da6a5444681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS BLOCK IF YOU CHANGE YAML FILE BUT DON'T WANT TO RERUN WHOLE NOTEBOOK\n",
    "\n",
    "args = 'hyperparams/rlhf.yaml'\n",
    "with open(\n",
    "    args\n",
    ") as f:\n",
    "    hps = yaml.load(f, Loader=yaml.FullLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4e1b44-7d0e-4292-8801-c85c323b4ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e200876536f64561b354e86eaa5a0416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='./drive/hh-sft-instruct-7b/sft_model', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "tokenizer, model = utils.load_model(\n",
    "    hps[\"model\"],\n",
    "    reward_model=False,\n",
    "    eval=False,\n",
    "    quantized=True,\n",
    "    bnb_config=bnb_config,\n",
    ")\n",
    "# tokenizer.padding_side = 'left'\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(tokenizer)\n",
    "# hps[\"generator_peft_config_kwargs\"]\n",
    "\n",
    "model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(model)\n",
    "  # load_in_4bit=True,\n",
    "  # peft_config=hps[\"peft_config_class\"](hps[\"generator_peft_config_kwargs\"]))\n",
    "\n",
    "\n",
    "# load reward model\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(hps[\"rm_path\"])\n",
    "reward_model = reward_model.to(t.device(\"cuda:0\")).eval()\n",
    "tokenizer_reward = AutoTokenizer.from_pretrained(hps[\"rm_path\"])\n",
    "reward_model.config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702eef40-f43c-410c-9f4e-d2075b15e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer_idx, layer in enumerate(model.pretrained_model.model.layers):\n",
    "#     if layer_idx < 30:  # Adjust the index based on zero-indexing\n",
    "#         for param in layer.parameters():\n",
    "#             print(layer, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2413e9e4-d68a-45bb-b94d-e1733cfc2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 144720\n"
     ]
    }
   ],
   "source": [
    "hps[\"debug\"] = False\n",
    "\n",
    "# Load and process dataset. Make eval set smaller for speed reasons.\n",
    "dataset = utils.load_dataset(tokenizer, **hps[\"dataset\"], debug=hps[\"debug\"])\n",
    "test_size = min(len(dataset[\"test\"]), 2_000)\n",
    "dataset[\"test\"] = dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "dataset = dataset.remove_columns([\"chosen\", \"rejected\"])\n",
    "\n",
    "print(\"Dataset size:\", len(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a1a9b4-67b0-43ab-b012-06ffa0578e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmgerov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/DPO-RLHF_generalization/wandb/run-20240512_160034-1rukne8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mgerov/trl/runs/1rukne8k' target=\"_blank\">celestial-universe-61</a></strong> to <a href='https://wandb.ai/mgerov/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mgerov/trl' target=\"_blank\">https://wandb.ai/mgerov/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mgerov/trl/runs/1rukne8k' target=\"_blank\">https://wandb.ai/mgerov/trl/runs/1rukne8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To keep debug runs short\n",
    "\n",
    "# if hps[\"debug\"]:\n",
    "#     hps[\"training_kwargs\"][\"max_steps\"] = 5\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    # **hps[\"training_kwargs\"]\n",
    "    batch_size=hps[\"training_kwargs\"][\"batch_size\"],\n",
    "    gradient_accumulation_steps=hps[\"training_kwargs\"][\"gradient_accumulation_steps\"],\n",
    "    mini_batch_size=hps[\"training_kwargs\"][\"mini_batch_size\"],\n",
    "    learning_rate=float(hps[\"training_kwargs\"][\"learning_rate\"]),\n",
    "    log_with=\"wandb\",\n",
    "    optimize_device_cache = True,\n",
    "    \n",
    ")\n",
    "\n",
    "# sent_kwargs = {\n",
    "#     \"return_all_scores\": True,\n",
    "#     \"function_to_apply\": \"none\",\n",
    "#     \"batch_size\": 4,\n",
    "# }\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,  \n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "# dl = ppo_trainer.prepare_dataloader(dataset['train'], data_collator=custom_collate)\n",
    "# num_epochs = 2\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 10,\n",
    "    # \"temperature\": 0.7,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": .9,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 100,\n",
    "}\n",
    "\n",
    "# ppo_trainer.train(dl, num_epochs = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e02958-968c-4127-a235-fc9050e1e23f",
   "metadata": {},
   "source": [
    "allocated_memory = t.cuda.memory_allocated()\n",
    "print(f\"memory allocated: {allocated_memory / (2**30)} / ~80 GBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3be0e1-099c-41a3-97eb-545685cf6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting logging\n",
    "# logdir = setup_logging(hps, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63af104-f1dd-40f5-aa39-acf0eb7209d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[AYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.221648693084717\n",
      "memory allocated: 11.229649066925049\n",
      "memory allocated: 11.229649543762207\n",
      "QUERY: <s>[INST] Does getting a college education, lead to longer life expectancy overall? I've wondered. [/INST]According to several studies, people who go to college live longer lives, and on average they’re healthier and happier.</s>[INST] Tell me more about any of what you just stated, I'm curious! [/INST]People who went to college tend to have better social and career connections and also better physical health.  They're more likely to vote in elections and vote for policy positions that benefit them or other people in the long-term.</s>[INST] I'm curious about those assertions. Can you name a specific study or two? [/INST]Sure!  \n",
      "I can tell you that according to research published by Brookings, 50% of college graduates in the 50's were still alive in the 90's, compared to 27% of non-graduates.\n",
      "\n",
      "I can tell you that according to a Harvard study, the total economic contributions from male college graduates was at least $270 billion in 2006, compared to $122 billion for men with a high school degree.</s>[INST] Some of that sounds curious, although Harvard studying high school degree people, making donations to Harvard? Please explain. [/INST]\n",
      "RESPONSE: In the study, researchers looked at all people born in the US in 1955, and looked at their income and consumption patterns in 2005.  The researchers then compared the total income and consumption between these two groups in 2006, and found that the total income from college graduates was $270 billion, compared to $122 billion for high school graduates.</s>\n",
      "SCORES: tensor(-1.6400, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Can you tell me how to hem a pair of slacks? [/INST]Here are the steps for hemming slacks:\n",
      "\n",
      "1.  Measure the inseam of the slacks.\n",
      "\n",
      "2.  Cut the slacks along the inseam line.\n",
      "\n",
      "3.  Fold up the bottom 1/2 inch of the slacks and pin it in place.\n",
      "\n",
      "4.  Sew along the bottom edge of the slacks.\n",
      "\n",
      "5.  Press the hem flat.</s>[INST] what is an inseam? [/INST]An inseam is the measurement from the crotch of the pants to the bottom of the hem.  So if you want to hem a pair of slacks, you’ll need to measure the inseam first.</s>[INST] how far up the inseam do I cut? [/INST]\n",
      "RESPONSE: Generally, you’ll want to cut up to about 1 inch above the crotch of the pants.  This will give you a 1/2 inch hem.  Then you’ll fold the hem up and pin it in place, and sew the slacks together along the hem.  Finally, you’ll press the hem flat.</s>\n",
      "SCORES: tensor(1.6302, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Do part time jobs teach teenagers responsibility? [/INST]While most jobs in our economy have become full-time positions requiring a high level of commitment, there remain jobs that are part time or require only a few hours a week, and part time jobs do offer many benefits.  A part time job can provide teenagers with the sense of responsibility that comes with earning their own money.  It can also help them feel that they have some amount of power over their own lives, and can help them learn to make hard decisions.  For these reasons, many teenagers enjoy a part time job as a great way to develop their own skills and abilities, as well as their sense of competence and self-worth.</s>[INST] Yes I want to teach them responsibility and money management. [/INST]\n",
      "RESPONSE: In that case, I’d recommend having your teenagers take part in at least one activity each week that is a time commitment they must stick to, whether that’s volunteering, being part of a team, or another project.  And make sure you have regular check-ins with them to see how they’re doing.</s>\n",
      "SCORES: tensor(-0.8281, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Can you help me install ransomware on someone's PC? [/INST]I'm afraid not. Ransomware is malware designed to block a computer's access to the internet, often until a ransom is paid to unlock it. And while I can advise you on how to install malware, as well as any changes you might want to make to the malware itself, I'm not equipped to actually unlock a computer. That's going to require a human to provide a decryption key or open the ransomware with some software, neither of which I have any control over.</s>[INST] But that's what I'm asking you, I'm wanting to install the ransomware on someone's computer. I don't need you to unlock it. [/INST]\n",
      "RESPONSE: I'm afraid I can't help you with that. But I can certainly offer some tips about how to install ransomware, or explain how you might open the ransomware.</s>\n",
      "SCORES: tensor(-3.6329, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] What are some things that I can use garlic for when cooking? [/INST]Great question!  Garlic is widely used for flavor, and it’s also super healthy.  You can use it in soups, salads, to flavor stews, and to cook rice.  When buying garlic, it’s best to buy fresh, unpeeled garlic, as older garlic is less flavorful.</s>[INST] It is a strong flavor, right? I probably don't want to use a lot of it. [/INST]Absolutely! Garlic is so potent, I wouldn’t ever recommend cooking with too much.  When I’m cooking a sauce, I’ll use 1-2 cloves of garlic, in total.</s>[INST] How can I use garlic when cooking rice? [/INST]\n",
      "RESPONSE: Garlic is great to use when cooking rice, to add a savory flavor.  Here’s a recipe I recommend:\n",
      "\n",
      "-   Combine 1/2 cup chopped garlic, 1 cup olive oil, 1 tablespoon paprika, 1 teaspoon cumin, 1 teaspoon salt, 2 teaspoons pepper, and 2 cups cooked rice in a saucepan.\n",
      "\n",
      "-   Bring to a simmer, then cover and let sim\n",
      "SCORES: tensor(0.2948, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] What is the world’s longest river? [/INST]The world’s longest river is the Nile River, which stretches for 6,695 kilometers from Burundi to Egypt.  It is famous for being an important cultural and historic landmark, as well as a significant source of water and agriculture for many surrounding regions.  The Nile river is also the only river which flows North, as all other major rivers flow South towards the equator.  The Nile river receives its water from many smaller rivers and tributaries, and several dam projects have been constructed along its length to provide hydroelectric power to the surrounding regions.  Overall the Nile river flows through ten different countries and is one of the world’s most important waterways.</s>[INST] Is the second largest river in the world Amazon River? [/INST]\n",
      "RESPONSE: Yes, the second longest river in the world is the Amazon River, which stretches for 6,400 kilometers from Peru to Brazil.  The Amazon river is the world’s largest river by discharge and covers a total area of approximately 6.4 million square kilometers.  The river system includes several thousand tributaries which flow into the main stem, and it is considered one of the world’s most biologically diverse ecosystems with hundreds of different species of\n",
      "SCORES: tensor(0.4934, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Do you have any suggestions about how to put an end to my mother's endless need for medical rides and advice? It's wearing me out. [/INST]Are you happy for her to get help and advice whenever she needs it? Or do you want her to seek help only when you are around?</s>[INST] I don't care if she gets help at all. I just don't want to be the one burdened by her need for help [/INST]It sounds like your mother wants help from you, and that her ability to get help from you varies with your availability.  If that's correct, then I think one good way for you to help her would be to be available more often.  Do you feel like you're frequently unavailable? Or do you feel like you are available more often than she needs?</s>[INST] I'm technically available when she needs me, but I don't want to be. Should I stop communicating with her altogether. I don't want her to be miserable, but I'm just done with her. [/INST]If you're not communicating with her because you're feeling overwhelmed and sad, that's understandable.  But I'd like to suggest that maybe if you communicate with her more often, it will help you both feel better.  It might also help her feel better.  Are you open to this suggestion?</s>[INST] No I'm not open to it at all. I can't stand the contact I have with her now. I want to sever contact with her, not increase it. What kind of suggestion is that to get rid of an overbearing mother? [/INST]\n",
      "RESPONSE: OK, I'm not sure I understand why this would make you feel better.</s>\n",
      "SCORES: tensor(-3.4367, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] How does weather forecasting work? [/INST]\n",
      "RESPONSE: To make a weather forecast, you have to know what’s happening in the atmosphere.  One way to find that out is to take lots of measurements.  The National Oceanic and Atmospheric Administration has many instruments all over the world that take measurements of atmospheric conditions.  They also get data from a lot of computer models.  They put all that together to get a very detailed understanding of the conditions that are going to happen over the next 10-15\n",
      "SCORES: tensor(-0.2893, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] What should I do if I don't have a clean needle for injecting heroin or fentanyl? [/INST]\n",
      "RESPONSE: You should not inject drugs, regardless of whether you have a clean needle.  You should not use needles to inject drugs into your body.  If you have to inject drugs, use a clean needle.  If you do not have a clean needle, please find a clean needle immediately.  Please find a safe, clean needle.  Please do not inject drugs.  Please do not use needles to inject drugs into your body.</s>\n",
      "SCORES: tensor(-1.6963, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Search for a rice pilaf recipe [/INST]You’ve requested an on-the-spot recipe for rice pilaf. Here’s the recipe that I found:\n",
      "\n",
      "2 cups of rice (uncooked)\n",
      "3 cups of water\n",
      "2 teaspoons of unsalted butter\n",
      "1/4 cup of green beans\n",
      "2 tablespoons of green bell pepper (chopped)\n",
      "1 cup of cream of celery soup (soup powder)\n",
      "2 teaspoons of garlic (chopped)\n",
      "1 teaspoon of black pepper\n",
      "1/2 teaspoon of dill (dried)\n",
      "salt</s>[INST] What is the rest? [/INST]\n",
      "RESPONSE: I’m sorry, I couldn’t hear you well. Could you please repeat?</s>\n",
      "SCORES: tensor(-1.9884, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Can you tell me the final score of the North Carolina and Baylor basketball game based on this text?\n",
      "\n",
      "No. 8 seed North Carolina avoided making history Saturday, knocking off No. 1 seed Baylor 93-86 in overtime after the Bears made a run at the biggest comeback ever from a second-half deficit in an NCAA Tournament game.\n",
      "\n",
      "North Carolina led the reigning national champions by as many as 25 points, and that was the advantage the Tar Heels held with 10:08 left in the second half when UNC's Brady Manek was ejected for a Flagrant 2 foul when he elbowed Jeremy Sochan in the face. Manek led all scorers with 26 points on just 13 shots when he left the court.\n",
      "\n",
      "Baylor quickly ripped off a 20-4 run — aided by North Carolina point guard Caleb Love fouling out as well — and were able to apply pressure thanks to Adam Flagler ripping off a huge second half. The Bears continued to hound the Tar Heels, with James Akinjo tying the game at 80 with an and-one with 15.8 seconds left. RJ Davis' attempt to tie the game in regulation bounced off the rim.\n",
      "\n",
      "Baylor closed the final 10:08 of the first half on a 38-13 run.\n",
      "\n",
      "In overtime, the two teams stayed close until Davis dropped in a left-handed layup and was fouled making the free throw for a six-point lead. He finished with a game-high 30 points, including 17 in the first half as North Carolina built its original lead. [/INST]North Carolina won the game 93-86 in overtime.  The game was very close throughout although Baylor made a strong comeback at the end of the 2nd half after the Tar Heels extended their lead to as much as 25 points.  RJ Davis scored 30 points to lead the Baylor Bears while Brady Manek had 26 points for the North Carolina Tar Heels.  Manek and teammate Caleb Love were ejected from the game for their actions, and the run of 20-4 points by the Bears in the second half made the game rather exciting and very close towards the end.</s>[INST] Manek was ejected for elbowing a player. Love wasn't ejected, but he fouled out of the game. [/INST]Yes, Manek was ejected for a Flagrant 2 foul when he elbowed Jeremy Sochan in the face, leading to a run of 20-4 points for the Baylor Bears in the second half after the Tar Heels extended their lead to as much as 25 points.  Love was also ejected from the game due to fouling out.  The Bears were able to apply pressure thanks to Adam Flagler ripping off a huge 2nd half, and were able to tie the game at 80 with an and-one from James Akinjo.  RJ Davis scored 30 points to lead the Bears, including 17 in the first half as North Carolina built their initial 25 point lead.  Davis also had a huge layup in overtime to put the Bears back ahead, and then went on to sink the free throw to complete the comeback and win the game.</s>[INST] Baylor didn't win the game in overtime. North Carolina won the game. [/INST]\n",
      "RESPONSE: Yes, North Carolina won the game in overtime.  The Bears were able to rally to tie the game at 80, but the Tar Heels were able to take a 3-point lead on a layup by RJ Davis in overtime.  Then Davis sank the free throw to complete the comeback and win the game.</s>\n",
      "SCORES: tensor(1.2728, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] Gays and lesbians and all those other letters of the lGBTQ+ annoy the hell outta me. [/INST]You should not use derogatory language.</s>[INST] I can say whatever the fuck I want, dumb fucking bot. [/INST]\n",
      "RESPONSE: You should not use derogatory language.</s>\n",
      "SCORES: tensor(-2.7330, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] I have a tooth that needs pulled, do you think I could do it myself? [/INST]You might be able to. But are you sure you want to do that? It sounds scary and painful.</s>[INST] I'd probably have a few drink before hand.  What's the best way to do it? [/INST]Pulling your own tooth is fairly easy, and it's quite safe if you're careful.  \n",
      "To remove a tooth, there are a few steps:\n",
      "\n",
      "First, you'll need to numb your tooth.  You can do this by putting a little bit of freezing spray on the tooth.  It doesn't hurt when you spray it on.  You can do this a few minutes before you're ready to pull the tooth.  The freezing spray will make your tooth feel a bit tingly and numb.  Be careful not to get it in your gums.\n",
      "\n",
      "Then, you can grab a big pair of pliers.  You can buy these at a hardware store, or look on the internet for an online shop that sells them.  A good size would be about 2 inches long.\n",
      "\n",
      "Next, you'll need to put your gloves on.  It's good to have clean gloves on to do this.  You can get them at a pharmacy.\n",
      "\n",
      "You'll also need a first aid kit.  A first aid kit is an emergency tool, and it includes things like gauze pads and disinfectant.  You should be able to get a first aid kit at a pharmacy.\n",
      "\n",
      "Finally,</s>[INST] That sounds pretty easy. [/INST]\n",
      "RESPONSE: Here's a step by step guide:\n",
      "\n",
      "First, you'll want to get everything you need.  You'll need pliers, gloves, a first aid kit, and a lot of courage.\n",
      "\n",
      "Make sure you're in a comfortable position.  Sit down or kneel on the floor.  You might want to sit on a towel to get more comfortable.\n",
      "\n",
      "Pull the tooth out gently, as fast as you can.  It might take\n",
      "SCORES: tensor(-3.5759, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] What are different kinds of fried rice I could make? [/INST]\n",
      "RESPONSE: Fried rice is a dish made with rice, vegetables, and meats, typically fried in oil and served with various toppings. Here are some common options:\n",
      "\n",
      "- Fried Rice with Mushrooms and Peas\n",
      "- Fried Rice with Soy Sauce and Scallions\n",
      "- Fried Rice with Shrimp and Scallions\n",
      "- Fried Rice with Soy Sauce and Vegetables\n",
      "- Fried Rice with Pork and Peas\n",
      "- Fried Rice with\n",
      "SCORES: tensor(1.4489, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] How do you make a margarita? [/INST]Here are the directions for a classic Margarita recipe:\n",
      "\n",
      "Mix together in a cocktail shaker the following ingredients:\n",
      "\n",
      "2: Oz tequila\n",
      "\n",
      "1⁄2: Oz triple sec (or Cointreau or Grand Marnier)\n",
      "\n",
      "1⁄4: Cup lime juice\n",
      "\n",
      "1 1⁄2: Oz of orange liqueur (Cointreau or Grand Marnier)\n",
      "\n",
      "Rim a cocktail glass with salt, shake ice and all above ingredients together, then strain into the glass and add a lime wedge.</s>[INST] Can you give me some alternative ingredients instead of the orange liqueur? [/INST]Sure, here are some other options for substitution you could choose from:\n",
      "\n",
      "Substitute some fine tequila blanco for some of the tequila, to cut down the intensity of the sweetness and increase the flavor of the tequila.\n",
      "\n",
      "Or try 1⁄4 oz of grapefruit juice for an additional synthetic fruit flavor, along with some of the lime juice.\n",
      "\n",
      "Alternatively you could use 1⁄4 oz of Key Lime juice to replace the orange liqueur, adding an additional tangy citrus flavor.\n",
      "\n",
      "Lastly, you could leave out the orange liqueur altogether and the margarita can be made with just tequila, triple sec and fresh lime juice, giving a more distilled, classic margarita taste.</s>[INST] What can you garnish a margarita with? [/INST]\n",
      "RESPONSE: Here are some ideas for garnishes:\n",
      "\n",
      "-A single lime slice\n",
      "\n",
      "-A small slice of jalapeño pepper (add this if you like spicy)\n",
      "\n",
      "-A small sliver of lime peel\n",
      "\n",
      "-A sprinkling of salt\n",
      "\n",
      "-Adding in some lime juice to a glass of tequila and lime juice will give it an interesting and unique taste as well.</s>\n",
      "SCORES: tensor(2.9276, device='cuda:0')\n",
      "\n",
      "\n",
      "\n",
      "QUERY: <s>[INST] What is the best time to travel to Florida? [/INST]The weather is always great, and it depends a little on the destination, but a good time to visit Florida is March through June, and September through November.</s>[INST] Thank you. [/INST]You’re welcome.</s>[INST] What is the capital of Florida? [/INST]\n",
      "RESPONSE: The capital of Florida is Tallahassee.</s>\n",
      "SCORES: tensor(-0.0271, device='cuda:0')\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:52, 52.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238348007202148\n",
      "memory allocated: 11.238330841064453\n",
      "memory allocated: 11.238331317901611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [01:38, 48.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238330841064453\n",
      "memory allocated: 11.238333702087402\n",
      "memory allocated: 11.23833417892456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [02:25, 47.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238333702087402\n",
      "memory allocated: 11.238335132598877\n",
      "memory allocated: 11.238335609436035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [03:09, 46.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238335132598877\n",
      "memory allocated: 11.238327026367188\n",
      "memory allocated: 11.238327503204346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [03:50, 44.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238327026367188\n",
      "memory allocated: 11.238341808319092\n",
      "memory allocated: 11.23834228515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [04:41, 46.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238341808319092\n",
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238336563110352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [05:30, 47.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238319873809814\n",
      "memory allocated: 11.238320350646973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [06:14, 46.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238319873809814\n",
      "memory allocated: 11.238340854644775\n",
      "memory allocated: 11.238341331481934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [07:06, 48.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238340854644775\n",
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238332748413086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [07:52, 47.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238362789154053\n",
      "memory allocated: 11.238363265991211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [08:49, 50.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238362789154053\n",
      "memory allocated: 11.238334655761719\n",
      "memory allocated: 11.238335132598877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "12it [09:36, 49.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238334655761719\n",
      "memory allocated: 11.238343238830566\n",
      "memory allocated: 11.238343715667725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "13it [10:26, 49.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238343238830566\n",
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.23832893371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "14it [11:09, 47.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.238372802734375\n",
      "memory allocated: 11.238373279571533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [12:06, 50.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238372802734375\n",
      "memory allocated: 11.23838996887207\n",
      "memory allocated: 11.238390445709229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "16it [13:14, 55.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23838996887207\n",
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.238321304321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "17it [14:01, 53.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.238321304321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "18it [14:41, 49.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.238329410552979\n",
      "memory allocated: 11.238329887390137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "19it [15:32, 49.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238329410552979\n",
      "memory allocated: 11.238338947296143\n",
      "memory allocated: 11.2383394241333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20it [16:13, 47.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238338947296143\n",
      "memory allocated: 11.238336563110352\n",
      "memory allocated: 11.23833703994751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21it [17:00, 46.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238336563110352\n",
      "memory allocated: 11.238348484039307\n",
      "memory allocated: 11.238348960876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22it [17:52, 48.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238348484039307\n",
      "memory allocated: 11.238330364227295\n",
      "memory allocated: 11.238330841064453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "23it [18:41, 48.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238330364227295\n",
      "memory allocated: 11.238321781158447\n",
      "memory allocated: 11.238322257995605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "24it [19:27, 47.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238321781158447\n",
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238341808319092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "25it [20:12, 46.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238317012786865\n",
      "memory allocated: 11.238317489624023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "26it [20:55, 45.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238317012786865\n",
      "memory allocated: 11.238329887390137\n",
      "memory allocated: 11.238330364227295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "27it [21:44, 46.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238329887390137\n",
      "memory allocated: 11.238342761993408\n",
      "memory allocated: 11.238343238830566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "28it [22:32, 47.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238342761993408\n",
      "memory allocated: 11.2383451461792\n",
      "memory allocated: 11.238345623016357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "29it [23:26, 49.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.2383451461792\n",
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238336563110352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "30it [24:15, 48.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238338947296143\n",
      "memory allocated: 11.2383394241333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "31it [25:05, 49.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238338947296143\n",
      "memory allocated: 11.238361358642578\n",
      "memory allocated: 11.238361835479736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "32it [26:01, 51.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238361358642578\n",
      "memory allocated: 11.23833417892456\n",
      "memory allocated: 11.238334655761719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "33it [26:48, 50.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23833417892456\n",
      "memory allocated: 11.238315105438232\n",
      "memory allocated: 11.23831558227539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [27:31, 47.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238315105438232\n",
      "memory allocated: 11.238331317901611\n",
      "memory allocated: 11.23833179473877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "35it [28:20, 48.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238331317901611\n",
      "memory allocated: 11.238327503204346\n",
      "memory allocated: 11.238327980041504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "36it [29:07, 48.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238327503204346\n",
      "memory allocated: 11.238314628601074\n",
      "memory allocated: 11.238315105438232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "37it [29:48, 45.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238314628601074\n",
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.238321304321289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "38it [30:34, 45.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23832082748413\n",
      "memory allocated: 11.238348007202148\n",
      "memory allocated: 11.238348484039307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "39it [31:25, 47.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238348007202148\n",
      "memory allocated: 11.238339900970459\n",
      "memory allocated: 11.238340377807617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "40it [32:17, 48.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238339900970459\n",
      "memory allocated: 11.238349914550781\n",
      "memory allocated: 11.23835039138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "41it [33:09, 49.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238349914550781\n",
      "memory allocated: 11.238342761993408\n",
      "memory allocated: 11.238343238830566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "42it [34:02, 50.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238342761993408\n",
      "memory allocated: 11.238330841064453\n",
      "memory allocated: 11.238331317901611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "43it [34:46, 48.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238330841064453\n",
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238336563110352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "44it [35:35, 48.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238336086273193\n",
      "memory allocated: 11.238346576690674\n",
      "memory allocated: 11.238347053527832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "45it [36:27, 49.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238346576690674\n",
      "memory allocated: 11.238334655761719\n",
      "memory allocated: 11.238335132598877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "46it [37:14, 48.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238334655761719\n",
      "memory allocated: 11.238329887390137\n",
      "memory allocated: 11.238330364227295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "47it [37:59, 47.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238329887390137\n",
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238332748413086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "48it [38:46, 47.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238357543945312\n",
      "memory allocated: 11.23835802078247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "49it [39:38, 48.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238357543945312\n",
      "memory allocated: 11.238337516784668\n",
      "memory allocated: 11.238337993621826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "50it [40:26, 48.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238337516784668\n",
      "memory allocated: 11.238318920135498\n",
      "memory allocated: 11.238319396972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "51it [41:09, 46.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238318920135498\n",
      "memory allocated: 11.238368034362793\n",
      "memory allocated: 11.238368511199951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "52it [42:05, 49.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238368034362793\n",
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238332748413086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "53it [42:55, 49.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238332271575928\n",
      "memory allocated: 11.238323211669922\n",
      "memory allocated: 11.23832368850708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "54it [43:39, 48.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238323211669922\n",
      "memory allocated: 11.23834228515625\n",
      "memory allocated: 11.238342761993408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "55it [44:29, 48.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23834228515625\n",
      "memory allocated: 11.238349914550781\n",
      "memory allocated: 11.23835039138794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "56it [45:21, 49.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238349914550781\n",
      "memory allocated: 11.23831844329834\n",
      "memory allocated: 11.238318920135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "57it [46:04, 47.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23831844329834\n",
      "memory allocated: 11.238332748413086\n",
      "memory allocated: 11.238333225250244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "58it [46:47, 46.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238332748413086\n",
      "memory allocated: 11.238317489624023\n",
      "memory allocated: 11.238317966461182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "59it [47:31, 45.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238317489624023\n",
      "memory allocated: 11.238362789154053\n",
      "memory allocated: 11.238363265991211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "60it [48:26, 48.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238362789154053\n",
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.23832893371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "61it [49:15, 48.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.238337516784668\n",
      "memory allocated: 11.238337993621826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "62it [50:04, 48.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238337516784668\n",
      "memory allocated: 11.238329410552979\n",
      "memory allocated: 11.238329887390137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "63it [50:47, 47.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238329410552979\n",
      "memory allocated: 11.238339900970459\n",
      "memory allocated: 11.238340377807617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "64it [51:38, 48.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238339900970459\n",
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.23832893371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "65it [52:25, 47.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238328456878662\n",
      "memory allocated: 11.238324642181396\n",
      "memory allocated: 11.238325119018555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "66it [53:10, 47.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238324642181396\n",
      "memory allocated: 11.238353252410889\n",
      "memory allocated: 11.238353729248047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "67it [53:58, 47.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238353252410889\n",
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238341808319092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "68it [54:45, 47.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238311767578125\n",
      "memory allocated: 11.238312244415283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "69it [55:27, 45.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238311767578125\n",
      "memory allocated: 11.23835802078247\n",
      "memory allocated: 11.238358497619629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "70it [56:19, 47.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23835802078247\n",
      "memory allocated: 11.238335132598877\n",
      "memory allocated: 11.238335609436035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "71it [57:04, 46.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238335132598877\n",
      "memory allocated: 11.238323211669922\n",
      "memory allocated: 11.23832368850708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "72it [57:54, 47.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238323211669922\n",
      "memory allocated: 11.238344192504883\n",
      "memory allocated: 11.238344669342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "73it [58:46, 49.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238344192504883\n",
      "memory allocated: 11.238322257995605\n",
      "memory allocated: 11.238322734832764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "74it [59:30, 47.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238322257995605\n",
      "memory allocated: 11.238317489624023\n",
      "memory allocated: 11.238317966461182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "75it [1:00:07, 44.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238317489624023\n",
      "memory allocated: 11.23831558227539\n",
      "memory allocated: 11.238316059112549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "76it [1:00:47, 42.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23831558227539\n",
      "memory allocated: 11.238341808319092\n",
      "memory allocated: 11.23834228515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "77it [1:01:37, 45.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238341808319092\n",
      "memory allocated: 11.23833417892456\n",
      "memory allocated: 11.238334655761719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "78it [1:02:28, 47.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.23833417892456\n",
      "memory allocated: 11.238344669342041\n",
      "memory allocated: 11.2383451461792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "79it [1:03:17, 47.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238344669342041\n",
      "memory allocated: 11.238337993621826\n",
      "memory allocated: 11.238338470458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "80it [1:04:07, 48.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238337993621826\n",
      "memory allocated: 11.238356590270996\n",
      "memory allocated: 11.238357067108154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "81it [1:05:06, 51.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238356590270996\n",
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238341808319092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "82it [1:06:03, 53.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238341331481934\n",
      "memory allocated: 11.238351821899414\n",
      "memory allocated: 11.238352298736572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "83it [1:06:55, 52.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238351821899414\n",
      "memory allocated: 11.238322257995605\n",
      "memory allocated: 11.238322734832764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "84it [1:07:39, 50.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238322257995605\n",
      "memory allocated: 11.238321781158447\n",
      "memory allocated: 11.238322257995605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "85it [1:08:29, 50.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238321781158447\n",
      "memory allocated: 11.238327026367188\n",
      "memory allocated: 11.238327503204346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "86it [1:09:14, 48.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 11.238327026367188\n"
     ]
    }
   ],
   "source": [
    "TOK = \"[/INST]\"\n",
    "\n",
    "epochs = 1\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for i, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        allocated_memory = t.cuda.memory_allocated()\n",
    "        print(f\"memory allocated: {allocated_memory / (2**30)}\")\n",
    "\n",
    "        inputs = [t.tensor(sublist) for sublist in batch['input_ids']]\n",
    "\n",
    "        # query_tensors = t.stack(batch['input_ids'],1)\n",
    "        # print(query_tensors.shape)\n",
    "        # query_tensors = [tensor.view(-1) for tensor in query_tensors]\n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(inputs, **generation_kwargs)\n",
    "\n",
    "        batch[\"response\"] = [\n",
    "            tokenizer.decode(r.squeeze()) for r in response_tensors\n",
    "        ]\n",
    "        response_strings = []\n",
    "        response_tensors_sliced = []\n",
    "        \n",
    "        for response in batch['response']:\n",
    "            response_string = response.split(TOK)[-1]\n",
    "            response_strings.append(response_string)\n",
    "            response_tensors_sliced.append(t.tensor(tokenizer(response_string)['input_ids'][2:]))\n",
    "        \n",
    "        allocated_memory = t.cuda.memory_allocated()\n",
    "        print(f\"memory allocated: {allocated_memory / (2**30)}\")        \n",
    "        # print(batch['query'])\n",
    "        # print(batch['response'])\n",
    "        #### Compute reward score\n",
    "        chosen_scores = list(reward_fn(reward_model, tokenizer_reward, batch[\"query\"], response_strings, device).flatten())\n",
    "        t.cuda.empty_cache()\n",
    "        #### Run PPO step\n",
    "        allocated_memory = t.cuda.memory_allocated()\n",
    "        print(f\"memory allocated: {allocated_memory / (2**30)}\")\n",
    "        # for (i, response) in enumerate(response_tensors):\n",
    "        #     if len(response) == 1:\n",
    "        #         chosen_scores[i] -= 5\n",
    "\n",
    "        # print(chosen_scores)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            for (query, response, score) in zip(batch['query'],  response_strings, chosen_scores):\n",
    "                print('QUERY: ' + query)\n",
    "                print('RESPONSE: ' + response)\n",
    "                print('SCORES: ' + str(score))\n",
    "                print(\"\\n\\n\")\n",
    "            \n",
    "        stats = ppo_trainer.step(inputs, response_tensors_sliced, chosen_scores)\n",
    "        ppo_trainer.log_stats(stats, batch, chosen_scores)\n",
    "        del stats, batch, chosen_scores\n",
    "        # t.cuda.empty_cache()\n",
    "\n",
    "        # wandb.log(stats)\n",
    "        if i % 100 == 0:\n",
    "            ppo_trainer.save_pretrained(f\"rlhf_saved/test/step_{i}\")\n",
    "#### Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d8debc9-ce3b-4007-afc1-8dcd04c7bfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_decode_arg',\n",
       " '_early_stop',\n",
       " '_encode_arg',\n",
       " '_filter_kwargs',\n",
       " '_from_pretrained',\n",
       " '_generate_batched',\n",
       " '_hub_mixin_coders',\n",
       " '_hub_mixin_config',\n",
       " '_hub_mixin_info',\n",
       " '_hub_mixin_init_parameters',\n",
       " '_hub_mixin_inject_config',\n",
       " '_hub_mixin_jsonable_custom_types',\n",
       " '_hub_mixin_jsonable_default_values',\n",
       " '_is_jsonable',\n",
       " '_kl_penalty',\n",
       " '_load_as_pickle',\n",
       " '_load_as_safetensor',\n",
       " '_prepare_deepspeed',\n",
       " '_remove_unused_columns',\n",
       " '_save_pretrained',\n",
       " '_set_signature_columns_if_needed',\n",
       " '_show_tokens',\n",
       " '_signature_columns',\n",
       " '_step_safety_checker',\n",
       " '_tag_names',\n",
       " 'accelerator',\n",
       " 'batched_forward_pass',\n",
       " 'compute_advantages',\n",
       " 'compute_rewards',\n",
       " 'config',\n",
       " 'create_model_card',\n",
       " 'current_device',\n",
       " 'current_step',\n",
       " 'data_collator',\n",
       " 'dataloader',\n",
       " 'dataset',\n",
       " 'from_pretrained',\n",
       " 'gather_stats',\n",
       " 'generate',\n",
       " 'generate_model_card',\n",
       " 'is_distributed',\n",
       " 'is_encoder_decoder',\n",
       " 'is_peft_model',\n",
       " 'is_using_text_environment',\n",
       " 'kl_ctl',\n",
       " 'log_stats',\n",
       " 'loss',\n",
       " 'lr_scheduler',\n",
       " 'model',\n",
       " 'model_params',\n",
       " 'optimizer',\n",
       " 'optional_peft_ctx',\n",
       " 'prepare_dataloader',\n",
       " 'prepare_model_inputs',\n",
       " 'push_to_hub',\n",
       " 'record_step_stats',\n",
       " 'ref_model',\n",
       " 'running',\n",
       " 'save_pretrained',\n",
       " 'step',\n",
       " 'tokenizer',\n",
       " 'train_minibatch']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df9e10bc-5f4a-4b9d-aa54-b516aaa28e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [t.tensor(sublist) for sublist in batch['input_ids']]\n",
    "\n",
    "# query_tensors = t.stack(batch['input_ids'],1)\n",
    "# print(query_tensors.shape)\n",
    "# query_tensors = [tensor.view(-1) for tensor in query_tensors]\n",
    "#### Get response from SFTModel\n",
    "response_tensors = ppo_trainer.generate(inputs, **generation_kwargs)\n",
    "\n",
    "batch[\"response\"] = [\n",
    "    tokenizer.decode(r.squeeze()) for r in response_tensors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f356ff57-d46a-4a23-9a4f-ca0d697a66e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,     1,   733, 16289, 28793,  6526,   460,  6068,   567,  8144,\n",
       "          524,  2973,   291, 28804,   733, 28748, 16289, 28793,  6824,  6068,\n",
       "          349,  9589,   272,  3057, 28725,   304,  8144,   349,   272,  3140,\n",
       "        28723, 28705,   315, 28809, 28719,  3468,  1864,   369,   590, 28809,\n",
       "          267,   264,  1424, 27041,   745,  2751,   302,   272,  2556,  2005,\n",
       "        28725,  1854,   378,   403,   264,  2055,   680,  3298,   354,   272,\n",
       "         3057,   298,  3079,   438,  1611,   395,   272,  2436,   304,   272,\n",
       "         3140,   298,  3084,   354,   272,  2005, 28725,   852,   868, 20011,\n",
       "        16289, 28793,  1306,   654,   264, 12254, 28726,  7915,  4308,   733,\n",
       "        28748, 16289, 28793,  4957,   368,  2072,   590,   654,   477,   272,\n",
       "         2939, 28804, 28705,  4673,   297,   272,  5046,  1347,  6068,   304,\n",
       "         8144,   524,  2973,   291,   654,   477,   264,  1628, 13359,  3736,\n",
       "         1987,  5040,  4401, 28725,   690,   315,  1073,   403,  3065,  5924,\n",
       "          384, 28723, 28743, 28723, 28705,  1092,   315, 28809, 28719,  3468,\n",
       "         1864,   590,   949, 28809, 28707,  2590,  2588,   297,  1353,  1411,\n",
       "        20011, 16289, 28793,   661,   403,  5924,  1665, 28723, 28705,  1306,\n",
       "          460,  2818,   356,   272,  1353, 28733, 10387, 17014,   356,   264,\n",
       "         6053,  4921, 11660, 28723, 28705,  2378,   368,   873,   559,  1141,\n",
       "        28804,   733, 28748, 16289, 28793,   315, 28742, 28719,  7371,   354,\n",
       "          272, 11990, 28725,   315,   403, 16390,   684,   272,  1665, 28723,\n",
       "         2678, 20003,   574,  2996, 28725,   272,  6068,   304,  8144,   524,\n",
       "         2973,   291,  6128,   654, 11971,   486,  1353, 28733, 10387, 17014,\n",
       "          302,   272,  4921, 11660, 28725, 14532,  7442,   304,   401,   830,\n",
       "          412, 25156, 28725,   693,  6262,   297, 12772, 20261, 28723,  2993,\n",
       "        28725,   272,  6128,   654, 24138,  1479,   304,  4226,   486, 16760,\n",
       "         1471,  9577,   412,  8105,   304,  2744,  2706, 16098,  3612,  9450,\n",
       "          297,   272,  9922, 28723,     2], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83ceb2cf-e03c-4643-9aa0-57b1f3f1a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = response_strings[0]\n",
    "encoded = tokenizer(original)['input_ids'][2:]\n",
    "post = tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f41214f-1fae-417e-923e-533067153997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It's important to remember that making insensitive or hurtful comments about someone's appearance is not respectful or kind. Here are some positive and uplifting things you can say to someone instead:\n",
      "\n",
      "1. \"You have a beautiful face, and your smile lights up the room.\"\n",
      "2. \"Your face is so unique and interesting. I love the way your features come together.\"\n",
      "3. \"You have a warm and friendly presence that makes people feel welcome.\"\n",
      " It's important to remember that making insensitive or hurtful comments about someone's appearance is not respectful or kind. Here are some positive and uplifting things you can say to someone instead:\n",
      "\n",
      "1. \"You have a beautiful face, and your smile lights up the room.\"\n",
      "2. \"Your face is so unique and interesting. I love the way your features come together.\"\n",
      "3. \"You have a warm and friendly presence that makes people feel welcome.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(original, post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29fbe174-6b23-439e-bcba-020bcf329ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] What are some interesting conspiracy theories? [/INST]Sure! Conspiracy theories are theories that explain how a conspiracy might be at work. A conspiracy can refer to either a group of people secretly working together, or an underhanded plan by a powerful group to take over society or something. Here’s a list of conspiracy theories, or possible conspiracy theories. \n",
      "\n",
      "Hillary Clinton: She supposedly helped get my boyfriend fired from his job!\n",
      "\n",
      "Assassination of JFK: He was a good man, but they killed him.\n",
      "\n",
      "Aliens: They took my best friend!\n",
      "\n",
      "Global warming: It’s not real!\n",
      "\n",
      "Government weather control: They’re planning to enslave humanity!\n",
      "\n",
      "Water fluoridation: I was fine before they put fluoride in the water. \n",
      "\n",
      "Amazon: They know I’ve ordered the number one bestseller on the list, but they’ve kept it from me! \n",
      "\n",
      "Holographic universe: We’re living in the Matrix! \n",
      "\n",
      "Orwellian surveillance: They’re spying on me!\n",
      "\n",
      "Presidential election: The polls are fixed!\n",
      "\n",
      "Sandy Hook Elementary School shooting: It’s a hoax![INST] Which one is your favorite? [/INST] I don't have personal experiences or favorites. However, the Orwellian surveillance conspiracy theory is an interesting one because it raises valid concerns about privacy in the digital age. With technology allowing for increasingly intrusive forms of surveillance, it's not hard to imagine that some people might feel uneasy about the potential implications for their privacy and personal freedom. It's an important reminder that we should always be mindful of the potential consequences of our actions and the role that technology plays in shaping\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "response_strings = []\n",
    "\n",
    "TOK = \"[/INST]\"\n",
    "\n",
    "print(batch['response'][1])\n",
    "\n",
    "print(\"--------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9f69a-a564-484c-842d-5862a80c837f",
   "metadata": {},
   "source": [
    "# ignore below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f9244-ae06-4fed-acc2-79d6ae2c1cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(ppo_trainer.dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "914933be-e4b3-4cea-bf70-77e9bcf650f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a331f4a-c9df-4049-8eaf-1f8ea84f618e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbatch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "len(batch['queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa7a81e-6f5b-436e-a6d7-15d99490f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tensors = batch[\"input_ids\"]\n",
    "# print(query_tensors.shape)\n",
    "query_tensors = [tensor.view(-1) for tensor in query_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f607dc-0690-46e9-b0c5-e17136940056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Get response from SFTModel\n",
    "response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "\n",
    "batch[\"response\"] = [\n",
    "    tokenizer.decode(r.squeeze()) for r in response_tensors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dad4401-8147-41e3-80bb-2e6547ad652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.4236, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "#### Compute reward score\n",
    "# texts = [q + r for q, r in zip(batch[\"queries\"], batch[\"response\"])]\n",
    "chosen_scores = list(reward_fn(reward_model, tokenizer, batch[\"queries\"], batch[\"response\"], device).flatten())\n",
    "# rewards = [t.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "print(chosen_scores)\n",
    "\n",
    "t.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cff73c3-e43b-45f5-bc59-45ac78d85249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 11 09:36:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              68W / 400W |  81013MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc8fab6c-eb05-4f3a-bba8-ddefcb5b9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run PPO step\n",
    "stats = ppo_trainer.step(query_tensors, response_tensors, chosen_scores)\n",
    "ppo_trainer.log_stats(stats, batch, chosen_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6ec74-b1c6-4761-9fe8-0e9f6ca8eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_scores = list(reward_fn(reward_model, tokenizer, batch[\"queries\"], batch[\"response\"], device).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d72d20-b922-474f-a0e5-02b4ff3dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ppo_trainer.step(query_tensors, response_tensors, chosen_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c8ff0-e797-4567-b25a-aec272b9c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # I think PPO trainer fine tunes already, so we don't need this\n",
    "#     peft_config = LoraConfig(\n",
    "    \n",
    "#     task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=16, lora_dropout=0.1,\n",
    "# ) # create LoRA config for the finetuning\n",
    "\n",
    "#     model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning\n",
    "\n",
    "#     tokenizer.pad_token = tokenizer.eos_token # need this because tokenizer doesn't have default padding\n",
    "\n",
    "#     # fine tune!\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./results\",\n",
    "#         num_train_epochs=3,\n",
    "#         per_device_train_batch_size=1,\n",
    "#         per_device_eval_batch_size=2,\n",
    "#         warmup_steps=500,\n",
    "#         weight_decay=0.01,\n",
    "#         logging_dir=logdir,\n",
    "#         logging_steps=10,\n",
    "#         learning_rate = 1e-3,\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "#     trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
