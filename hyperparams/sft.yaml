model: mistralai/Mistral-7B-Instruct-v0.2

dataset:
    name: Anthropic/hh-rlhf
    # Dahoas/synthetic-instruct-gptj-pairwise
    data_dir: default  # Use both harmfuless and helpful splits
training_algorithm: sft


training_kwargs:
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 8
    max_steps: 20
    learning_rate: 0.0001