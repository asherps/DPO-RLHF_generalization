# Model hyperparameters
model: mistralai/Mistral-7B-Instruct-v0.2

peft_config_class: !!python/name:peft.tuners.lora.LoraConfig
peft_config_kwargs:
    r: 16
    lora_alpha: 16
    lora_dropout: 0.05

# Dataset hyperparameters
dataset:
    name: Dahoas/synthetic-instruct-gptj-pairwise
    data_dir: default  # Use both harmfuless and helpful splits

# Training hyperparameters
training_algorithm: dpo
training_algorithm_kwargs:
    beta: 0.1

training_kwargs:
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 8
    max_steps: 10000
    learning_rate: 0.0001

tags: []