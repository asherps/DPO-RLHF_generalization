# Model hyperparameters
model: mistralai/Mistral-7B-Instruct-v0.2

peft_config_class: !!python/name:peft.tuners.lora.LoraConfig
peft_config_kwargs:
    r: 16
    lora_alpha: 16
    lora_dropout: 0.05

# Dataset hyperparameters
dataset:
    name: UCL-DARK/sequential-instructions
    data_dir: default  # Use both harmfuless and helpful splits

dataset_name: sequential-instructions
# Training hyperparameters
training_algorithm: reward_model
training_algorithm_kwargs: {}

training_kwargs:
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 8
    gradient_accumulation_steps: 8
    max_steps: 5000
    learning_rate: 0.0001

tags: []